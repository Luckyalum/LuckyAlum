[{"title":"银行家算法（C语言实现）","date":"2017-06-03T16:00:00.000Z","path":"2017/06/04/银行家算法（C语言实现）/","text":"简介 银行家算法（Banker’s Algorithm）是一个避免死锁（Deadlock）的著名算法，是由艾兹格·迪杰斯特拉在1965年为T.H.E系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。 算法实现数据结构进程个数n资源类数m可利用资源向量Available含有m个元素的数组，其中的每一个元素代表一类可利用的资源数目。如果Available[j]=K，则表示系统中现有Rj类资源K个。最大需求矩阵Maxn×m的矩阵，它定义了系统中n个进程中的每一个进程对m类资源的最大需求。如果Max[i,j]=K，则表示进程i需要Rj类资源的最大数目为K。分配矩阵Allocationn×m的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果Allocation[i,j]=K，则表示进程i当前已分得Rj类资源的 数目为K。需求矩阵Needn×m的矩阵，用以表示每一个进程尚需的各类资源数。如果Need[i,j]=K，则表示进程i还需要Rj类资源K个，方能完成其任务。Need[i,j]=Max[i,j]-Allocation[i,j] 算法原理我们可以把操作系统看作是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。为保证资金的安全，银行家规定：(1) 当一个顾客对资金的最大需求量不超过银行家现有的资金时就可接纳该顾客；(2) 顾客可以分期贷款，但贷款的总数不能超过最大需求量；(3) 当银行家现有的资金不能满足顾客尚需的贷款数额时，对顾客的贷款可推迟支付，但总能使顾客在有限的时间里得到贷款；(4) 当顾客得到所需的全部资金后，一定能在有限的时间里归还所有的资金.操作系统按照银行家制定的规则为进程分配资源，当进程首次申请资源时，要测试该进程对资源的最大需求量，如果系统现存的资源可以满足它的最大需求量则按当前的申请量分配资源，否则就推迟分配。当进程在执行中继续申请资源时，先测试该进程本次申请的资源数是否超过了该资源所剩余的总量。若超过则拒绝分配资源，若能满足则按当前的申请量分配资源。 算法实现初始化由用户输入数据，分别对可利用资源向量矩阵AVAILABLE、最大需求矩阵MAX、分配矩阵ALLOCATION、需求矩阵NEED赋值。银行家算法在避免死锁的方法中，所施加的限制条件较弱，有可能获得令人满意的系统性能。在该方法中把系统的状态分为安全状态和不安全状态，只要能使系统始终都处于安全状态，便可以避免发生死锁。银行家算法的基本思想是分配资源之前，判断系统是否是安全的；若是，才分配。它是最具有代表性的避免死锁的算法。设编号为ID的进程提出请求new_request，定义（关于向量比较、运算的定义，代码实现部分给出，这里不再赘述）：i=new_request-&gt;id为该进程的编号，new_request-&gt;req_src为该进程此次所请求的资源向量，则银行家算法按如下规则进行判断：1.如果new_request-&gt;req_src &lt;= Need[i] 则转2；否则，出错。2..如果new_request-&gt;req_src &lt;= Available[i]，则转3；否则，等待。3.系统试探分配资源，修改相关数据：Available[i] -= new_request-&gt;req_src ;Allocation[i] += new_request-&gt;req_src;Need[i] -= new_request-&gt;req_src;4.系统执行安全性检查，如安全，则分配成立；否则试探险性分配作废，系统恢复原状，进程等待。安全性检查算法1.设置两个工作向量Work 记录系统当前可用资源量，初值为Available;finish 记录所有进程是否已被执行, 初值为长度为n，值均为False的向量。2.从进程集合中找到一个满足下述条件的进程，finish == False;Need &lt;= Work;如找到，执行3；否则，执行4。3.假设进程获得资源，可顺利执行，直至完成，从而释放资源。Work += Allocation;Finish=True;执行24.如所有的进程finish= True，则表示安全；否则系统不安全。 代码实现（C语言）首先，将需要的变量定义为全局变量12345678910111213int n; //进程数int m; //资源类数int *Available; //可使用资源向量int **Max; //最大需求矩阵int **Allocation; //分配矩阵int **Need; //需求矩阵bool safe = False;typedef struct&#123; int id; //进程ID int *req_src; //进程此次申请资源&#125;Request;Request* new_request; 如上，用到了Bool型变量，因此要定义123#define True 1#define False 0typedef int bool; 下面列出了我们将要写的函数：12345678void initial(); //初始化n,m,Available等的函数 void request(); //提出请求void process(); //处理bool safe_detect(); //安全性检测/*向量运算函数*/bool vector_compare(int *a, int *b, int len); void vector_add(int *a, int *b, int len);void vector_sub(int *a, int *b, int len); 首先给出几个向量运算函数的定义：定义a和b为两个等长向量,a &gt;= b 表示 a 中的每个元素都大于相应位置上的 b 的元素；a += b 表示 a 中的每个元素增加相应位置上的 b 的元素的值；a -= b 表示 a 中的每个元素都大于相应位置上的 b 的元素的值；例：a = [1,2,3];b = [1,1,1];则a &gt;= b;a += b; //a=[2,3,4]a -= b; //a=[0,1,2]12345678910111213141516171819202122232425262728293031bool vector_compare(int *a, int *b, int len) // If vector a &gt;= vector b, return True&#123; int i = 0; while(i&lt;len) &#123; if(*(a+i)&lt;*(b+i)) return False; i++; &#125; return True;&#125;void vector_add(int *a, int *b, int len) //vector a += vector b&#123; int i = 0; while(i&lt;len) &#123; *(a+i) += *(b+i); i++; &#125; &#125;void vector_sub(int *a, int *b, int len) //vector a -= vector b&#123; int i = 0; while(i&lt;len) &#123; *(a+i) -= *(b+i); i++; &#125; &#125; 下面按算法步骤给出 initial(), request(), process(), safe_request()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129void initial()&#123; int i; int j; printf(\"请输入进程数:\\n\"); scanf(\"%d\",&amp;n); printf(\"请输入资源类数:\\n\"); scanf(\"%d\",&amp;m); printf(\"请输入可使用资源向量:\\n\"); Available = (int*)malloc(sizeof(int)*m); for(i=0; i&lt;m; i++) scanf(\"%d\",&amp;Available[i]); printf(\"请输入最大需求矩阵:\\n\"); Max = (int**)malloc(sizeof(int*)*n); for(i=0; i&lt;n; i++) &#123; Max[i] = (int*)malloc(sizeof(int)*m); for(j=0; j&lt;m; j++) scanf(\"%d\",&amp;Max[i][j]); &#125; printf(\"请输入分配矩阵:\\n\"); Allocation = (int**)malloc(sizeof(int*)*n); for(i=0; i&lt;n; i++) &#123; Allocation[i] = (int*)malloc(sizeof(int)*m); for(j=0; j&lt;m; j++) scanf(\"%d\",&amp;Allocation[i][j]); &#125; Need = (int**)malloc(sizeof(int*)*n); for(i=0;i&lt;n;i++) &#123; Need[i] = (int *)malloc(sizeof(int)*m); for(j=0;j&lt;m;j++) Need[i][j] = Max[i][j] - Allocation[i][j]; &#125;&#125;void request()&#123; int i,id; new_request = (Request*)malloc(sizeof(Request)); new_request-&gt;req_src = (int*)malloc(sizeof(int)*m); printf(\"请输入进程的ID\\n\"); scanf(\"%d\",&amp;id); new_request-&gt;id = id - 1; printf(\"请输入进程申请资源向量\\n\"); for(i=0; i&lt;m; i++) scanf(\"%d\",&amp;new_request-&gt;req_src[i]);&#125;void process()&#123; int i = new_request-&gt;id; if(vector_compare(Need[i],new_request-&gt;req_src,m)) &#123; if(vector_compare(Available,new_request-&gt;req_src,m)) &#123; vector_sub(Available,new_request-&gt;req_src,m); vector_add(Allocation[i],new_request-&gt;req_src,m); vector_sub(Need[i],new_request-&gt;req_src,m); safe_detect(); &#125; else &#123; printf(\"程序所申请资源大于系统当前所剩资源，推迟执行!\\n\"); return; &#125; &#125; else &#123; printf(\"程序所申请资源大于该程序所需资源，无法执行!\\n\"); return; &#125; if(safe) &#123; printf(\"系统安全,进程可以执行!\\n\"); return; &#125; else &#123; printf(\"系统不安全,进程无法执行!\\n\"); vector_add(Available,new_request-&gt;req_src,m); vector_sub(Allocation[i],new_request-&gt;req_src,m); vector_add(Need[i],new_request-&gt;req_src,m); return; &#125; &#125;bool safe_detect()&#123; int *work = Available; bool *finish = (bool*)malloc(sizeof(bool)*n); int i; //初始化finish for(i=0; i&lt;n; i++) finish[i] = False; for(i=0; i&lt;n; i++) &#123; if(finish[i]==False&amp;&amp;vector_compare(work,Need[i],m)) &#123; printf(\"尝试执行第%d进程\\n\",i+1); vector_add(work,Allocation[i],m); //尝试执行该进程，释放资源 finish[i] = True; i = -1; //尝试分配后，从头查找是否还有可以执行的进程，考虑到i++，故此处为-1 &#125; &#125; for(i=0; i&lt;n; i++) if(finish[i]==False) break; if(i==n) safe = True; else safe = False;&#125; 最后完整源码见这里","tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://baoxizhao.com/tags/Operating-System/"}]},{"title":"R in Linux 安装及library的安装","date":"2017-05-22T16:00:00.000Z","path":"2017/05/23/R in Linux 安装及library的安装/","text":"1.安装命令很简单（网上有些很复杂，需要上官网下载等，诚然直接apt-get版本可能有点旧，但是简单啊）12$:sudo apt-get install r-base$:R 2.library的安装（这个可以和平台无关）1&gt;install.packages(\"mypackages\")","tags":[{"name":"R","slug":"R","permalink":"http://baoxizhao.com/tags/R/"}]},{"title":"Machine Learning学习笔记之中文文本分类","date":"2017-05-19T16:00:00.000Z","path":"2017/05/20/MachineLearning学习笔记之中文文本分类/","text":"文本挖掘与文本分类的概念文本挖掘：是指从大量的文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些识更好的组织信息以便将来参考。搜索和信息检索（IR）：存储和文本文档的检索，包括搜索引擎个关键字搜索文本聚类：使用聚类方法，对词汇、片段、段落或文件进行分组和归类文本分类：对片段、段落或文件进行分组和归类，在使用数据挖掘分类方法的基础上，经过训练的标记示例模型。Web挖掘：在互联网上进行数据和文本的挖掘，并特别关注网络的规模和相互的联系。信息抽取（IE）：从非结构化文本中识别与提取有关的事实和关系：从非结构化或半结构化文本中抽取结构化数据的过程。自然语言处理（NLP）：将语言作为一种有意义、有规则的符号系统，从底层解析和理解语言的任务（例如词性的标注）；目前的技术方法主要从语法、语义的角度发现语言最本质的结构和所表达的意义。概念的提取：把单词和短语按语义分成意义相似的组 文本分类项目文本分类的一般步骤： （1）预处理：去除文本的噪声信息，例如HTML标签、文本的格式转换、检测句子边界等。 （2）中文分词：使用中文分词器为文本分词，并去除停用词。 （3）构建词向量空间：统计文本词频，生成文本的词向量空间。 （4）权重策略—TF-IDF方法：使用TF-IDF发现特征词，并抽取为反应文档主题的特征。 （5）分类器：使用算法训练分类器。 （6）评价分类结果：分类器的测试结果分析 文本预处理 1.选择处理的文本的范围 2.建立分类文本语料库 中文文本分类语料库下载地址为：点击这里（但我阅读本书的时候，此网址已失效，我找的这个） 3.文本格式转换 Python去除HTML标签，一般使用lxml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#coding:utf-8from lxml import etree,htmlimport chardet#HTML文件路径，以及读取文件path = '1.html'content = open(path,\"rb\").read()print type(content)page = html.document_fromstring(content)#解析文件text = page.text_content()#去除所有标签# print type(text)# print chardet.detect(text)print text #输出去除标签后的解析结果 ``` 4.**检测句子边界**：标记句子的结束### 2.2.2 中文分词介绍 文本的结构化表示简单分为四大类：词向量空间模型、主题模型、依存句法的树表示、RDF的图表示 jieba分词简单的样例代码``` python#jieba分词#coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\",cut_all=False)print \"Default Mode:\",\" \".join(seg_list)#默认切分seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\",cut_all=True)print \"Full Mode:\",\" /\".join(seg_list)#默认切分#搜索引擎模式seg_list = jieba.cut_for_search(\"小明1995年毕业于北京清华大学\")print \"search:\",\" /\".join(seg_list)#默认切分#词性标注import jieba.posseg as psegwords =pseg.cut(\"我爱北京天安门\")for w in words: print w.word,w.flagPrefix dict has been built succesfully. 小明 1995 年 毕业 于 北京 清华大学Full Mode: 小 /明 /1995 /年 /毕业 /于 /北京 /清华 /清华大学 /华大 /大学search: 小明 /1995 /年 /毕业 /于 /北京 /清华 /华大 /大学 /清华大学我 r爱 v北京 ns天安门 ns 本项目创建分词后，语料路径为Root\\train_corpus_seg.1）设置字符集，并导入jieba分词包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')#定义两个函数读取和保存文件def savefile(savepath,content):#保存至文件 fp = open(savepath,\"wb\") fp.write(content) fp.close()def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return content#整个语料库分词的主程序#未分词分类语料库路径#分词后的分类语料库路径corpus_path = \"WorkSpace/TextClassification/train_corpus_small/\"seg_path = \"WorkSpace/TextClassification/train_corpus_seg/\"#获取corpus_path下的所有子目录catelist = os.listdir(corpus_path)for mydir in catelist: #拼出分类子目录的路径 class_path = corpus_path+mydir+\"/\" #拼出分词后的语料分类目录 seg_dir = seg_path+mydir+\"/\" #是否存在目录，如果没有则创建 if not os.path.exists(seg_dir): os.makedirs(seg_dir) #获得类别目录下的所有文件 file_list = os.listdir(class_path) #遍历类别目录下的所有文件 for file_path in file_list: #拼出文件名的全路径 fullname = class_path + file_path #读取文件的内容 content = readfile(fullname).strip() #删除换行和多余的空格 content = content.replace(\"\\r\\n\",\"\").strip() #为文件的内容分词 content_seg = jieba.cut(content) #将处理后的文件保存到分词后的语目录 savefile(seg_dir+file_path,\"\".join(content_seg))print u\"中文语料分析结束！！！\" 在实际应用中，为了后续的生成空间模型的方便，这些分词后的文本信息还要转化为文本向量信息并对象化需要引入Scikit-Learn库的Bunch的数据结构：12345678910111213141516171819202122232425262728293031323334353637383940import sysimport osimport jiebafrom sklearn.datasets.base import Bunch#Bunch类import cPickle as pickle#Bunch类提供了一种key,value的对象形式#target_name:所有分类集名称列表#label每个文件的分类标签列表#filename:文件路径#contents：分词后文件词向量形式def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentbunch = Bunch(target_name = [],label = [],filename = [],contents = [])#将分好词的文本文件转换并持久化为Bunch类形式的代码如下：#分词语料Bunch对象持久化文件路径wordbag_path = \"WorkSpace/TextClassification/train_word_bag/train_set.dat\"seg_path = \"WorkSpace/TextClassification/train_corpus_seg/\" #分词后分类语料库路径catelist = os.listdir(seg_path) #bunch.target_name.extend(catelist) #按类别信息保存到Bunch对象中for mydir in catelist: class_path = seg_path + mydir + \"/\" file_list = os.listdir(class_path) for file_path in file_list: fullname = class_path + file_path bunch.label.append(mydir)#保存当前文件的分类标签 bunch.filename.append(fullname)#保存当前文件路径 bunch.contents.append(readfile(fullname).strip())#保存文件词向量#Bunch对象的持久化file_obj = open(wordbag_path,\"wb\")pickle.dump(bunch,file_obj)file_obj.close()print u\"构建文本对象结束！！！\" Scikit-Learn库介绍点击这里 向量空间模型可以从http://www.threedweb.cn/thread-1294-1-1.html下载(同样，这里网址失效，我使用的是这个)读取停用词：123#读取停用词列表代码stopword_path = \"WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines() 权重策略：TD-IDF方法含义：如果某个词或短语在一篇文章中出现的频率越高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。词频（Term Frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（Term Count）的归一化，以防止它偏向长的文件。对于在某一特定文件里的词语来说，它的重要性可以表示为：其中，分子是该词在文件中出现的次数，分母是文件中所有字词的出现次数之和：逆向文件频率（Inverse Document Frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到：其中|D|：语料库中的文件总数。j：包含词语的文件数目。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用1+j作为分母TF-IDF = TF *IDF2.代码的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import sysimport osfrom sklearn.datasets.base import Bunchimport cPickle as picklefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformer #TF-IDF向量转换类from sklearn.feature_extraction.text import TfidfVectorizer#配置utf-8输出环境reload(sys)sys.setdefaultencoding('utf-8')def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentstopword_path = \"WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines()#1.读取和写入Bunch对象的函数def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#写入Bunch对象def writebunchobj(path,bunchobj): file_obj = open(path,\"wb\") pickle.dump(bunchobj,file_obj) file_obj.close()#从训练集生成TF-IDF向量词袋#2.导入分词后的词向量Bunch对象path = \"WorkSpace/TextClassification/train_word_bag/train_set.dat\"#词向量空间保存路径bunch = readbunchobj(path)#3.构建TF-IDF向量空间模型tfidfspace = Bunch(target_name = bunch.target_name,label = bunch.label,\\ filename = bunch.filename,tdm = [],vocabulary = &#123;&#125;)#使用TfidfVectorizer初始化向量空间模型vectorizer = TfidfVectorizer(stop_words = stpwrdlst,sublinear_tf = True,max_df = 0.5)transform = TfidfTransformer()#该类会统计每个词语放入Tf-IDF权重#4.文本转化为词频矩阵：单独保存字典文件tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)tfidfspace.vocabulary = vectorizer.vocabulary_#5.创建词袋的持久化space_path = \"WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"#词向量词袋的保存路径writebunchobj(space_path,tfidfspace) 使用朴素贝叶斯分类模块最常用的文本分类方法有KNN最近邻算法、朴素贝叶斯算法和支持向量机算法。一般来说，KNN最近邻算法的原理最简单，分类精度尚可，但速度最慢，朴素贝叶斯算法对于短文文本分类效果最好，精度最高；支持向量机算法的优势是支持线性不可分的情况，精度上取中。测试集随机抽取子训练集中的文档集合，每个分类取10个文档，过滤掉1KB以下的文档。训练步骤与训练集相同，首先是分词，之后生成文件词向量文件，直至生成词向量模型。不同的是，在训练词向量模型时，需要加载训练集词袋，将测试集产生的词向量映射到训练集词袋的词典中，生成向量空间模型。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#coding:utf-8import sysimport osfrom sklearn.datasets.base import Bunchimport cPickle as picklefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformer #TF-IDF向量转换类from sklearn.feature_extraction.text import TfidfVectorizer#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentstopword_path = \"../WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines()#1.读取和写入Bunch对象的函数def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunchdef writebunchobj(path,bunchobj): file_obj = open(path,\"wb\") pickle.dump(bunchobj,file_obj) file_obj.close()#2.导入分词后的词向量Bunch对象path = \"../WorkSpace/TextClassification/test_word_bag/test_set.dat\"#词向量空间保存路径bunch = readbunchobj(path)#3.构建测试集TF-IDF向量空间testspace = Bunch(target_name = bunch.target_name,label = bunch.label,filenames = \\ bunch.filename,tdm = [],vocabulary = &#123;&#125;)#4.导入训练集词袋trainbunch = readbunchobj(\"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\")#5.使用TfidfVectorizer初始化向量空间模型vectorizer = TfidfVectorizer(stop_words = stpwrdlst,sublinear_tf = True ,max_df = 0.5,\\ vocabulary = trainbunch.vocabulary) #使用训练集词袋向量transformer = TfidfTransformer()testspace.tdm = vectorizer.fit_transform(bunch.contents)testspace.vocabulary = trainbunch.vocabulary#6.创建词袋的持久化space_path = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"writebunchobj(space_path,testspace) 测试集数据的处理：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#（1）设置字符集，并导入jieba分词包# coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')#定义两个函数读取和保存文件def savefile(savepath,content):#保存至文件 fp = open(savepath,\"wb\") fp.write(content) fp.close()def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return content#整个语料库分词的主程序#未分词分类语料库路径#分词后的分类语料库路径corpus_path = \"../WorkSpace/TextClassification/test_corpus/\"seg_path = \"../WorkSpace/TextClassification/test_corpus_seg/\"#获取corpus_path下的所有子目录catelist = os.listdir(corpus_path)for mydir in catelist: #拼出分类子目录的路径 class_path = corpus_path+mydir+\"/\" #拼出分词后的语料分类目录 seg_dir = seg_path+mydir+\"/\" #是否存在目录，如果没有则创建 if not os.path.exists(seg_dir): os.makedirs(seg_dir) #获得类别目录下的所有文件 file_list = os.listdir(class_path) #遍历类别目录下的所有文件 for file_path in file_list: #拼出文件名的全路径 fullname = class_path + file_path #读取文件的内容 content = readfile(fullname).strip() #删除换行和多余的空格 content = content.replace(\"\\r\\n\",\"\").strip() #为文件的内容分词 content_seg = jieba.cut(content) #将处理后的文件保存到分词后的语目录 savefile(seg_dir+file_path,\"\".join(content_seg))print u\"中文语料分析结束！！！\"import sysimport osimport jiebafrom sklearn.datasets.base import Bunch#Bunch类import cPickle as pickle'''Bunch类提供了一种key,value的对象形式target_name:所有分类集名称列表label每个文件的分类标签列表filename:文件路径contents：分词后文件词向量形式'''def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentbunch = Bunch(target_name = [],label = [],filename = [],contents = [])#将分好词的文本文件转换并持久化为Bunch类形式的代码如下：#分词语料Bunch对象持久化文件路径wordbag_path = \"../WorkSpace/TextClassification/test_word_bag/test_set.dat\"seg_path = \"../WorkSpace/TextClassification/test_corpus_seg/\" #分词后分类语料库路径catelist = os.listdir(seg_path) #bunch.target_name.extend(catelist) #按类别信息保存到Bunch对象中for mydir in catelist: class_path = seg_path + mydir + \"/\" file_list = os.listdir(class_path) for file_path in file_list: fullname = class_path + file_path bunch.label.append(mydir)#保存当前文件的分类标签 bunch.filename.append(fullname)#保存当前文件路径 bunch.contents.append(readfile(fullname).strip())#保存文件词向量#Bunch对象的持久化file_obj = open(wordbag_path,\"wb\")pickle.dump(bunch,file_obj)file_obj.close()print u\"构建文本对象结束！！！\" 执行朴素贝叶斯训练：12345678910111213141516171819202122232425262728293031import cPickle as picklefrom sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法包def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#导入训练向量空间trainpath = \"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"train_set = readbunchobj(trainpath)#导入测试集向量空间testpath = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"test_set = readbunchobj(testpath)#应用朴素贝叶斯#alpha:0.001 alpha越小，迭代次数越多，精度越高clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm,train_set.label)#预测分类结果predicted = clf.predict(test_set.tdm)total = len(predicted)rate = 0for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted): if flabel != expct_cate: rate += 1 print file_name,u\":实际类别:\",flabel,u\"--&gt;预测类别:\",expct_cate#精度print \"error rate:\",float(rate)*100/float(total),\"%\" 输出结果：12345678910111213141516171819202122232425262728293031import cPickle as picklefrom sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法包def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#导入训练向量空间trainpath = \"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"train_set = readbunchobj(trainpath)#导入测试集向量空间testpath = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"test_set = readbunchobj(testpath)#应用朴素贝叶斯#alpha:0.001 alpha越小，迭代次数越多，精度越高clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm,train_set.label)#预测分类结果predicted = clf.predict(test_set.tdm)total = len(predicted)rate = 0for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted): if flabel != expct_cate: rate += 1 print file_name,u\":实际类别:\",flabel,u\"--&gt;预测类别:\",expct_cate#精度print \"error rate:\",float(rate)*100/float(total),\"%\" 输出结果： 分类结果评估（1）召回率（Recall Rate，也叫查全率）：是检索出相关文档数和文档库中所有相关文档的比率，衡量的是检索系统的查全率 召回率（Recall） = 系统检索到的相关文件/系统所有相关的文件的总数 （2）准确率（Precision，也成称为精度）：是检索出的相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。 准确率（Precision） = 系统检索到的相关文件/系统所有检索到的文档总数 （3）Fβ-Mesure(又称为F-Score）：是机器学习领域常用的评价标准，计算公式： 其中，β是参数，p是准确率，R是召回率 当β=1时，就是最常见的F1-Mesure了： 文本分类结果评估，代码如下：12345678910import numpy as npfrom sklearn import metrics#定义分类精确度def metrics_result(actual,predict): print u\"精度:&#123;0:.3f&#125;\".format(metrics.precision_score(actual,predict)) print u\"召回:&#123;0:.3f&#125;\".format(metrics.recall_score(actual,predict)) print u\"f1-score:&#123;0:.3f&#125;\".format(metrics.f1_score(actual,predict))metrics_result(test_set.label,predicted) 输出结果如下： 精度:0.881 召回:0.862 f1-score:0.860 资料来源及相关版权所有：《机器学习算法原理与编程实践》 郑捷","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://baoxizhao.com/tags/Machine-Learning/"}]},{"title":"CentOS7+Tomcat+Eclipse发布JavaWeb项目","date":"2017-04-07T16:00:00.000Z","path":"2017/04/08/CentOS7+Tomcat+Eclipse发布JavaWeb项目/","text":"准备工作必需安装的有-jdk(1.7.0_79)-Tomcat(8.5.13)可选安装的有-MySQL(CentOS7用的是Maria) 1.安装JDK下载、解压、配置路径12345wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz mv jdk-7u79-linux-x64.tar.gz /usr/local/jdkcd /usr/localtar -zxvf jdk vi /etc/profile 在最后添加123export JAVA_HOME=/usr/local/jdk/jdk1.7.0_79(JDK的解压目录) export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar 使文件立即生效1source /etc/profile 检查是否安装成功1java -version 2.下载并启动Tomcat81234wget http://apache.fayea.com/tomcat/tomcat-8/v8.5.13/bin/apache-tomcat-8.5.13.tar.gzcd usr/local/ tar -zxvf apache-tomcat-8.5.13.tar.gz/usr/local/apache-tomcat-8.5.13/bin/startup.sh 网速慢可以使用镜像 wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-8/v8.5.13/bin/apache-tomcat-8.5.13.tar.gz 3.安装MySQLCentOS7用的是Maria，所以你想安装MySQL可能会收到错误No package mysql-server available.解决办法:Centos 7 comes with MariaDB instead of MySQL. MariaDb is a open source equivalent to MySQL and can be installed with yum -y install mariadb-server mariadb. If you must have mysql you need to add the mysql-community repo sudo rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm and then you can install MySQLl like you normally do然后你就可以像MySQL一样用它了。 4.部署JavaWeb项目我使用的是Eclipse，于是右键要发布的项目，选择export，选择javaweb项目，然后就会得到WAR文件了上传文件使用的是Xftp5,点击新建会话,填入服务器的ip,服务我选择的是sftp,端口选择选的是22（这些可能需要你根据实际情况选择），输入用户和密码，就可以连上了，把刚才的WAR文件拖拽到服务器的Tomcat的webapps目录下就行了，有可能需要你服务器开启FTP服务（我没遇到）。","tags":[{"name":"CentOS7","slug":"CentOS7","permalink":"http://baoxizhao.com/tags/CentOS7/"},{"name":"Tomcat","slug":"Tomcat","permalink":"http://baoxizhao.com/tags/Tomcat/"},{"name":"Java","slug":"Java","permalink":"http://baoxizhao.com/tags/Java/"}]},{"title":"python使用国内镜像","date":"2017-04-05T16:00:00.000Z","path":"2017/04/06/python使用国内镜像/","text":"python安装包推荐使用Anaconda,但是有的时候，Anaconda里没有所需的包，就只能用pip命令了，如1pip install python-qt5 但是有时下载速度会很慢，毕竟网站在国外，此时可以考虑用镜像。 国内镜像http://pypi.douban.com/simple/ 豆瓣http://mirrors.aliyun.com/pypi/simple/ 阿里http://pypi.hustunique.com/simple/ 华中理工大学http://pypi.sdutlinux.org/simple/ 山东理工大学http://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学https://pypi.tuna.tsinghua.edu.cn/simple 清华 食用方法：1.临时食用 pip install -i http://mirrors.aliyun.com/pypi/simple/ python-qt5 此时可能会报错，提示你如果信任该镜像网站的话，可以使用https,把http改成https就行了。2.配制成默认的（没亲自试过）在你的“C:\\Users\\你的用户名\\”目录下创建“pip”目录，“pip”目录下创建“pip.ini”文件（注意：以UTF-8 无BOM格式编码） [global] index-url=http://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com 注意：trusted-host 选项为了避免麻烦是必须的，否则使用的时候会提示不受信任，或者添加“–trusted-host=mirrors.aliyun.com”选项；注意：有网页提示需要创建或修改配置文件（Linux的文件在~/.pip/pip.conf，windows在%HOMEPATH%\\pip\\pip.ini），至少Windows7下“%HOMEPATH%\\pip\\pip.ini”这个目录是不起作用的。======================参考文档信息======================版权声明：非商用自由转载-保持署名-注明出处署名(BY) ：testcs_dn(微wx笑)文章出处：无知人生，记录点滴","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"}]},{"title":"python openCV图像处理之提取轮廓","date":"2017-03-17T16:00:00.000Z","path":"2017/03/18/python openCV图像处理之提取轮廓/","text":"1.导入需要的包1import cv2 这里只做最简单的处理，所以只需导入cv2 2.载入图片1img = cv2.imread('kindle.jpg') 3.图片处理因为后面的cv2.findContours函数要求传入的为二值图像，所以需要先对图片进行处理12gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #转换为灰度图像ret, binary = cv2.threshold(gray,127,255,cv2.THRESH_BINARY) #转换为二值图像 4.轮廓提取1_,contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) #提取轮廓 contours即为轮廓的像素集合（可能有多个轮廓，需要找到自己要提取的轮廓）具体参数可查doc.opencv.org 5.显示图像cv2.drawContours(img,contours,-1,(0,0,255),2) #图片，轮廓集合，第几个轮廓 ，画笔颜色，画笔宽度 cv2.imshow(\"img\", img) cv2.waitKey(0) 6.所有代码及结果#-*- coding=utf8 -*- import cv2 import numpy as np img = cv2.imread('kindle.jpg') gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #转换为灰度图像 ret, binary = cv2.threshold(gray,127,255,cv2.THRESH_BINARY) #转换为二值图像 _,contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)#提取轮廓 cv2.drawContours(img,contours,-1,(0,0,255),2) cv2.imshow(\"img\", img) cv2.waitKey(0)","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"http://baoxizhao.com/tags/opencv/"},{"name":"图像处理","slug":"图像处理","permalink":"http://baoxizhao.com/tags/图像处理/"}]},{"title":"python openCV图像处理之角度变换","date":"2017-03-17T16:00:00.000Z","path":"2017/03/18/python openCV图像处理之角度变换/","text":"import cv2 import numpy as np img = cv2.imread('demo.jpg') conners = np.array([[873,1322],[1973,1864],[2645,3152],[857,2568]], dtype = \"float32\") #矩形(比如：书)的顶点 tl,tr,br,bl canvas = np.array([[0,0],[500,0],[500,500],[0,500]], dtype = \"float32\") #输出文件的大小 M = cv2.getPerspectiveTransform(conners,canvas) result = cv2.warpPerspective(img,M,(0,0)) cv2.imshow(\"img\", result) cv2.waitKey(0)","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"http://baoxizhao.com/tags/opencv/"},{"name":"图像处理","slug":"图像处理","permalink":"http://baoxizhao.com/tags/图像处理/"}]},{"title":"常见排序算法","date":"2017-03-12T16:00:00.000Z","path":"2017/03/13/2017-3-13-常见排序算法/","text":"1.冒泡排序算法：重复经过未排序数组，每次比较两个相邻元素，如果它们值相反，就交换它们的值代码实现：123456789101112static void BubbleSort(int[] a, int n) &#123; for(int i=1; i&lt;n; i++) for(int j=0; j&lt;n-i;j++) if(a[j]&gt;a[j+1]) &#123; int tmp; tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; &#125; &#125; 2.插入排序算法：每一步都将一个待排数据按其大小插入到已经排序的数据中的适当位置,直到全部插入完毕代码实现：1234567891011static void InsertSort(int[] a, int n) &#123; for(int i=0; i&lt;n; i++) &#123; int m = a[i]; int j; for(j=i; j&gt;0&amp;&amp;m&lt;a[j-1]; j--) a[j] = a[j-1]; a[j] = m; &#125; &#125; 3.选择排序算法：每一趟排序从序列中未排好序的那些元素中选择一个值最小的元素，然后将其与这些未排好序的元素的第一个元素交换位置代码实现： static void SelectSort(int[] a, int n) { for(int i=0; i&lt;n; i++) { int min = a[i]; int index = i; int tmp; for(int j=i+1; j&lt;n; j++) if(a[j]&lt;min) { min = a[j]; index = j; } tmp = a[i]; a[i] = min; a[index] = tmp; } } 4.快速排序算法：先从数列中取出一个数作为基准数，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边，再对左右区间重复上述过程，直到各区间只有一个数代码实现： static void QuickSort(int[] a, int left, int right) { if(left&lt;right) { int i = left; int j = right; int m = a[i]; while(i&lt;j) { while(i&lt;j&amp;&amp;a[j]&gt;=m) j--; if(i&lt;j) a[i++] = a[j]; while(i&lt;j&amp;&amp;a[i]&lt;=m) i++; if(i&lt;j) a[j--] = a[i]; } a[i] = m; QuickSort(a, left, i-1); QuickSort(a, i+1, right); } } 最后","tags":[{"name":"算法","slug":"算法","permalink":"http://baoxizhao.com/tags/算法/"},{"name":"排序","slug":"排序","permalink":"http://baoxizhao.com/tags/排序/"}]},{"title":"Linux基础学习","date":"2017-02-04T16:00:00.000Z","path":"2017/02/05/2017-2-5-linux学习/","text":"Linux基础学习（第一部分 Linux的规则与安装）主要参考来源-鸟哥的Linux私房菜基础学习篇（第三版） 1.搭建环境由于一些原因（就是穷，买不起服务器），通过虚拟机来学习Linux，但是这种环境很多硬件都是仿真的，因此有条件的话建议还是直接装Linux或利用多重引导装双系统。 相关配置： 主机系统： Windows 7 64位 虚拟机软件 VMare Workstation 12.5.2 虚拟机系统 CentOS 7安装： 首先安装VMare，然后下载CentOS 7镜像文件，VMare安装CentOS也没什么要特别注意的地方。 2.登录系统 CentOS 7有图形界面X Window（GNOME,GNOME classical等）和命令行界面。 两种模式的切换：Ctrl+Alt+F2~F7 命令行界面（tty2~tty7）Ctrl+Alt+F1 图形界面（tty1）注意：不同的distribution不同，如CentOS 5.x tty7为图形界面 如果使用命令行模式启动的Linux，可以使用1$start x 不过该命令并非万能 终端界面登录Linuxlocalhost login: baoxzh password: Last login: Wed Feb 8 19:30:30 on:0 [baoxzh@localhost~]$ 注意输入的密码不会显示出来 3.基础命令 命令行模式下执行命令基本格式： command [-options] parameter1 parameter2…即 命令或可执行文件+选项+参数1+参数2+…注意：大小写区分 简单命令： 显示日期命令 date 显示日历命令 cal 不妨试试 $ cal 9 1752(和历法有关) 计算器 bc 基本命令： 改变目录 cd (change directory) 选择用户 su (switch user) 删除文件 rm (remove) 显示文件属性 ls 退出 exit 重要热键： Tab：命令补全或文件补齐 Ctrl+C：终止当前命令 Ctrl+D：离开，相当于exit 求助命令（Manual） man page 和 info page 关机、重启 shutdown reboot halt poweroff 切换执行等级 run level 0：关机 run level 3：纯命令行模式 run level 5：含有图形界面模式 run level 6：重启","tags":[{"name":"Linux","slug":"Linux","permalink":"http://baoxizhao.com/tags/Linux/"}]},{"title":"Hello World!","date":"2017-01-31T09:06:25.000Z","path":"2017/01/31/2017-01-31-hello-world/","text":"个人博客终于完成了！关于jekyll建立blog的教程有很多，这里只记下动手时遇到的几点注意事项：1.gem源由于gem源 ruby.taobao.org已不再维护，故要改为gems.ruby-china.org即 http://gems.ruby-china.org2.bundler错误1jekyll error bundler 需安装 bundler，即1$ gem install jekyll bundler 3.各种包错误问题安装别人主题时可能需要下载所需的包,例如：1Could not find minitest-5.8.3 in any of the sources 输入1$ gem install minitest -v 5.8.3 有时还会遇到后面版本不向前兼容（不熟悉ruby，不知道为啥），我采取的措施是卸载不需要的那个版本,例如1$ gem uninstall minitest -v 5.9.3 安装包可以通过gem命令，也可在rubygems.org上下载安装，举例下载了minitest.gem文件，cd到下载目录后1$ gem install --local minitest.gem 4.最后jekyll在windows下配置比较繁琐，因此后来使用了Hexo，点击教程。完成之后，更新博客只需在git bash 博客目录下输入：12$ hexo g $ hexo d 2017/05/23 更新:最近换了Deepin linux,来更新一下Linux下Hexo的搭建需要 node.js、npm、gitnode.js及npm的安装见这按照官网上的方法接下来只需1$ npm install -g hexo-cli 然而这时会报错，我的解决办法是使用淘宝源安装淘宝源1$ npm install -g cnpm --registry=https://registry.npm.taobao.org 安装Hexo1$ cnpm install -g hexo-cli 接下来配置githubOver!","tags":[{"name":"jekyll","slug":"jekyll","permalink":"http://baoxizhao.com/tags/jekyll/"},{"name":"hexo","slug":"hexo","permalink":"http://baoxizhao.com/tags/hexo/"}]}]